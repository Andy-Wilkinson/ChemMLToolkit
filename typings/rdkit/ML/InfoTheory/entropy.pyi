"""
This type stub file was generated by pyright.
"""

""" Informational Entropy functions

  The definitions used are the same as those in Tom Mitchell's
  book "Machine Learning"

"""
_log2 = ...
def PyInfoEntropy(results): # -> int:
  """ Calculates the informational entropy of a set of results.

  **Arguments**

    results is a 1D Numeric array containing the number of times a
    given set hits each possible result.
    For example, if a function has 3 possible results, and the
      variable in question hits them 5, 6 and 1 times each,
      results would be [5,6,1]

  **Returns**

    the informational entropy

  """
  ...

def PyInfoGain(varMat): # -> float | Literal[0]:
  """ calculates the information gain for a variable

    **Arguments**

      varMat is a Numeric array with the number of possible occurrences
        of each result for reach possible value of the given variable.

      So, for a variable which adopts 4 possible values and a result which
        has 3 possible values, varMat would be 4x3

    **Returns**

      The expected information gain
  """
  ...

if hascEntropy:
  InfoEntropy = ...
  InfoGain = ...
else:
  InfoEntropy = ...
  InfoGain = ...
